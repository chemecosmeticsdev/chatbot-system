name: Performance Validation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance validation daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - quick
          - comprehensive
          - load_test
          - ci_cd
      concurrent_users:
        description: 'Number of concurrent users for load test'
        required: false
        default: '5'
        type: string
      test_duration:
        description: 'Test duration in seconds'
        required: false
        default: '30'
        type: string

env:
  NODE_VERSION: '18'
  PERFORMANCE_TARGET_MS: 200
  FAIL_ON_REGRESSION: true
  MAX_REGRESSION_PERCENT: 10

jobs:
  performance-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: pgvector/pgvector:pg15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_chatbot
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for baseline comparison

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          npm run build

      - name: Setup test database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_chatbot
        run: |
          # Install pgvector extension
          PGPASSWORD=postgres psql -h localhost -U postgres -d test_chatbot -c "CREATE EXTENSION IF NOT EXISTS vector;"

          # Run database migrations
          npm run db:migrate || echo "Migrations may not exist yet"

          # Seed test data for performance testing
          npm run db:seed:performance || echo "Performance seeding not configured"

      - name: Run performance validation
        id: performance-test
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_chatbot
          BAWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          BAWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DEFAULT_REGION: ap-southeast-1
          BEDROCK_REGION: us-east-1
          NODE_ENV: test
        run: |
          # Determine test type
          TEST_TYPE="${{ github.event.inputs.test_type || 'comprehensive' }}"
          CONCURRENT_USERS="${{ github.event.inputs.concurrent_users || '5' }}"
          TEST_DURATION="${{ github.event.inputs.test_duration || '30' }}"

          echo "Running $TEST_TYPE performance validation..."

          # Create performance test script
          cat > performance-test.js << 'EOF'
          const { PerformanceValidationService } = require('./lib/validation/performance-validation');
          const { CICDPerformanceIntegration } = require('./lib/validation/ci-cd-integration');
          const { getNeonPool } = require('./lib/neon');

          async function runPerformanceTest() {
            const pool = getNeonPool();
            const performanceService = new PerformanceValidationService(pool);
            const cicdIntegration = new CICDPerformanceIntegration(performanceService);

            const testType = process.env.TEST_TYPE || 'comprehensive';
            const commitHash = process.env.GITHUB_SHA || 'unknown';
            const branch = process.env.GITHUB_REF_NAME || 'unknown';
            const pipelineId = process.env.GITHUB_RUN_ID || 'manual';

            try {
              let result;

              switch (testType) {
                case 'quick':
                  result = await performanceService.validateSearchPerformance(
                    ['quick test query 1', 'quick test query 2'],
                    'ci-test-chatbot',
                    'ci-test-session'
                  );
                  break;

                case 'load_test':
                  result = await performanceService.validateUnderLoad({
                    concurrent_users: parseInt(process.env.CONCURRENT_USERS || '5'),
                    test_duration_seconds: parseInt(process.env.TEST_DURATION || '30'),
                    queries_per_user: 10,
                    ramp_up_seconds: 5,
                    dataset_size: 'medium',
                    query_complexity: 'moderate'
                  });
                  break;

                case 'ci_cd':
                  result = await cicdIntegration.runCICDPerformanceValidation(
                    pipelineId,
                    commitHash,
                    branch
                  );
                  break;

                case 'comprehensive':
                default:
                  const [validation, benchmarks, loadTest] = await Promise.all([
                    performanceService.validateSearchPerformance(
                      [
                        'authentication setup guide',
                        'database configuration',
                        'user management',
                        'API integration',
                        'troubleshooting common issues'
                      ],
                      'ci-test-chatbot',
                      'ci-test-session'
                    ),
                    performanceService.runPerformanceBenchmark(['small', 'medium']),
                    performanceService.validateUnderLoad({
                      concurrent_users: 3,
                      test_duration_seconds: 20,
                      queries_per_user: 8,
                      ramp_up_seconds: 3,
                      dataset_size: 'small',
                      query_complexity: 'moderate'
                    })
                  ]);

                  result = {
                    validation,
                    benchmarks,
                    loadTest,
                    overall_success: validation.overall_compliance && loadTest.meets_requirements
                  };
                  break;
              }

              // Output results for GitHub Actions
              console.log('=== PERFORMANCE TEST RESULTS ===');
              console.log(JSON.stringify(result, null, 2));

              // Generate CI/CD artifacts
              if (testType === 'ci_cd') {
                const summary = cicdIntegration.generateGitHubActionsSummary(result);
                const exports = cicdIntegration.exportPerformanceData(result);

                // Write summary to GitHub step summary
                const fs = require('fs');
                fs.writeFileSync(process.env.GITHUB_STEP_SUMMARY || 'step-summary.md', summary);

                // Write artifacts
                fs.writeFileSync('performance-results.xml', exports.junit_xml);
                fs.writeFileSync('performance-data.json', exports.performance_json);
                fs.writeFileSync('performance-metrics.txt', exports.metrics_prometheus);

                // Set outputs for GitHub Actions
                console.log(`::set-output name=performance_passed::${result.performance_tests_passed}`);
                console.log(`::set-output name=regression_detected::${result.regression_detected}`);
                console.log(`::set-output name=avg_response_time::${result.benchmarks[0]?.avg_response_time_ms || 0}`);
              } else {
                // Handle other test types
                const success = result.overall_success !== undefined ? result.overall_success :
                                result.overall_compliance !== undefined ? result.overall_compliance :
                                result.meets_requirements !== undefined ? result.meets_requirements : false;

                console.log(`::set-output name=performance_passed::${success}`);

                const avgResponseTime = result.avg_response_time_ms ||
                                       result.validation?.avg_response_time_ms ||
                                       result.benchmarks?.[0]?.avg_response_time_ms || 0;

                console.log(`::set-output name=avg_response_time::${avgResponseTime}`);
              }

              await pool.end();

              // Exit with appropriate code
              if (testType === 'ci_cd') {
                process.exit(result.performance_tests_passed ? 0 : 1);
              } else {
                const success = result.overall_success !== undefined ? result.overall_success :
                                result.overall_compliance !== undefined ? result.overall_compliance :
                                result.meets_requirements !== undefined ? result.meets_requirements : false;
                process.exit(success ? 0 : 1);
              }

            } catch (error) {
              console.error('Performance test failed:', error);
              console.log(`::set-output name=performance_passed::false`);
              console.log(`::set-output name=error::${error.message}`);
              process.exit(1);
            }
          }

          runPerformanceTest();
          EOF

          # Run the performance test
          TEST_TYPE="$TEST_TYPE" CONCURRENT_USERS="$CONCURRENT_USERS" TEST_DURATION="$TEST_DURATION" node performance-test.js

      - name: Upload performance artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.run_id }}
          path: |
            performance-*.xml
            performance-*.json
            performance-*.txt
            performance-*.md
          retention-days: 30

      - name: Comment on PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              // Read the performance summary if it exists
              let summary = '## 📊 Performance Test Results\n\n';

              if (fs.existsSync('performance-summary.md')) {
                summary = fs.readFileSync('performance-summary.md', 'utf8');
              } else {
                summary += `- **Test Type**: ${{ github.event.inputs.test_type || 'comprehensive' }}\n`;
                summary += `- **Status**: ${{ steps.performance-test.outputs.performance_passed == 'true' ? '✅ PASSED' : '❌ FAILED' }}\n`;
                summary += `- **Average Response Time**: ${{ steps.performance-test.outputs.avg_response_time || 'N/A' }}ms\n`;
                summary += `- **Target**: <${{ env.PERFORMANCE_TARGET_MS }}ms\n\n`;

                if ('${{ steps.performance-test.outputs.error }}') {
                  summary += `**Error**: ${{ steps.performance-test.outputs.error }}\n\n`;
                }

                summary += `See [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed results.`;
              }

              // Post comment on PR
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            } catch (error) {
              console.error('Failed to post PR comment:', error);
            }

      - name: Update performance baseline
        if: github.ref == 'refs/heads/main' && steps.performance-test.outputs.performance_passed == 'true'
        run: |
          # Commit updated performance baseline if tests passed on main branch
          if [ -f ".performance-baseline.json" ]; then
            git config --local user.email "action@github.com"
            git config --local user.name "GitHub Action"
            git add .performance-baseline.json
            git commit -m "Update performance baseline [skip ci]" || echo "No baseline changes to commit"
            git push || echo "Failed to push baseline update"
          fi

      - name: Create performance issue on failure
        if: failure() && github.ref == 'refs/heads/main'
        uses: actions/github-script@v7
        with:
          script: |
            const title = `🚨 Performance Validation Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Performance Test Failure

            **Commit**: ${{ github.sha }}
            **Branch**: ${{ github.ref_name }}
            **Workflow**: ${{ github.workflow }}
            **Run ID**: ${{ github.run_id }}

            ### Test Details
            - **Test Type**: ${{ github.event.inputs.test_type || 'comprehensive' }}
            - **Average Response Time**: ${{ steps.performance-test.outputs.avg_response_time || 'N/A' }}ms
            - **Target**: <${{ env.PERFORMANCE_TARGET_MS }}ms
            - **Regression Detected**: ${{ steps.performance-test.outputs.regression_detected || 'Unknown' }}

            ### Error
            \`\`\`
            ${{ steps.performance-test.outputs.error || 'Unknown error occurred' }}
            \`\`\`

            ### Action Required
            - [ ] Investigate performance degradation
            - [ ] Review recent changes that may impact performance
            - [ ] Run optimization if needed
            - [ ] Update performance targets if requirements changed

            **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            `;

            // Check if issue already exists for today
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: ['performance', 'ci'],
              state: 'open'
            });

            const today = new Date().toISOString().split('T')[0];
            const existingIssue = issues.data.find(issue =>
              issue.title.includes('Performance Validation Failed') &&
              issue.title.includes(today)
            );

            if (!existingIssue) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['performance', 'ci', 'high-priority']
              });
            }

      - name: Performance test summary
        if: always()
        run: |
          echo "=== PERFORMANCE VALIDATION SUMMARY ==="
          echo "Test Type: ${{ github.event.inputs.test_type || 'comprehensive' }}"
          echo "Status: ${{ steps.performance-test.outputs.performance_passed == 'true' && '✅ PASSED' || '❌ FAILED' }}"
          echo "Average Response Time: ${{ steps.performance-test.outputs.avg_response_time || 'N/A' }}ms"
          echo "Target: <${{ env.PERFORMANCE_TARGET_MS }}ms"
          echo "Regression Detected: ${{ steps.performance-test.outputs.regression_detected || 'N/A' }}"
          echo "Commit: ${{ github.sha }}"
          echo "Branch: ${{ github.ref_name }}"

          if [ "${{ steps.performance-test.outputs.performance_passed }}" != "true" ]; then
            echo "❌ Performance validation failed!"
            echo "Error: ${{ steps.performance-test.outputs.error || 'Unknown error' }}"
            exit 1
          else
            echo "✅ Performance validation passed!"
            exit 0
          fi